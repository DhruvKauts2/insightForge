# LogFlow - Distributed Log Aggregation System

A production-grade distributed log aggregation and analysis platform built with Python, Kafka, Elasticsearch, and PostgreSQL.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-required-blue.svg)](https://www.docker.com/)

## üéØ Overview

LogFlow is a **real-time log aggregation system** that collects, processes, stores, and analyzes logs from multiple microservices. It features a RESTful API, intelligent alerting, caching, rate limiting, and full authentication - all containerized and production-ready.

### Key Features

- üöÄ **Real-time Processing** - Stream logs through Kafka for scalable ingestion
- üîç **Advanced Search** - Full-text search with Elasticsearch
- üìä **Metrics & Analytics** - Real-time aggregations and statistics
- üö® **Smart Alerting** - Rule-based alerts with multiple notification channels
- üîê **Authentication** - JWT-based security with role-based access
- ‚ö° **Redis Caching** - 10-100x performance improvement on cached queries
- üö¶ **Rate Limiting** - Prevent API abuse and ensure fair usage
- üê≥ **Fully Containerized** - Docker Compose deployment
- üìñ **Interactive API Docs** - Swagger UI at `/docs`

---

## üìã Table of Contents

- [Quick Start](#-quick-start)
- [Architecture](#-architecture)
- [Features](#-features-in-detail)
- [API Endpoints](#-api-endpoints)
- [Authentication](#-authentication--security)
- [Alerting System](#-alerting-system)
- [Caching](#-caching)
- [Rate Limiting](#-rate-limiting)
- [Database Management](#-database-management)
- [Monitoring](#-monitoring)
- [Development](#-development)
- [Testing](#-testing)
- [Deployment](#-deployment)
- [Troubleshooting](#-troubleshooting)

---

## üöÄ Quick Start

### Prerequisites

- Docker Desktop (with Docker Compose)
- Git
- 8GB RAM minimum
- 10GB free disk space

### Installation
```bash
# 1. Clone the repository
git clone <your-repo-url>
cd logflow

# 2. Start all services
./scripts/start-all-docker.sh

# Wait ~30 seconds for services to initialize
```

### Verify Installation
```bash
# Check system health
curl http://localhost:8000/health | python3 -m json.tool

# View API documentation
open http://localhost:8000/docs
```

**Expected Output:**
```json
{
  "status": "healthy",
  "timestamp": "2025-10-24T08:30:00.000000",
  "services": {
    "elasticsearch": "green",
    "database": "healthy",
    "redis": "healthy"
  }
}
```

### Default Credentials

**Admin Account:**
- Username: `admin`
- Password: `admin123`
- ‚ö†Ô∏è **Change in production!**

---

## üèóÔ∏è Architecture
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Log Sources    ‚îÇ (Multiple Services)
‚îÇ  (Generators)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ Logs
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Log Shipper   ‚îÇ (Kafka Producer)
‚îÇ   (Producer)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Kafka      ‚îÇ (Message Queue)
‚îÇ  (Stream Queue) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Log Processor  ‚îÇ (Kafka Consumer)
‚îÇ   (Consumer)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Elasticsearch   ‚îÇ (Search & Storage)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   REST API      ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ PostgreSQL   ‚îÇ (Metadata)
‚îÇ  (FastAPI)      ‚îÇ     ‚îÇ  (Alerts DB) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Redis (Caching)
         ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ Alert Engine (Background)
```

**Components:**
- **Log Generator**: Simulates multi-service logs
- **Kafka**: Message streaming and buffering
- **Elasticsearch**: Log storage and search
- **PostgreSQL**: Users, alerts, configuration
- **Redis**: Caching and rate limiting
- **REST API**: FastAPI-based interface
- **Alert Engine**: Background alert processing

---

## ‚ú® Features in Detail

### 1. Log Aggregation Pipeline

- **Multi-Service Support**: Aggregate logs from unlimited services
- **Real-time Processing**: Stream processing with Kafka
- **Scalable**: Handle millions of logs per day
- **Fault Tolerant**: Kafka ensures no log loss

### 2. Advanced Search
```bash
# Search by query
GET /api/v1/logs/search?query=error

# Filter by service
GET /api/v1/logs/search?service=payment-service

# Filter by log level
GET /api/v1/logs/search?level=ERROR

# Combine filters
GET /api/v1/logs/search?service=auth-service&level=WARN&query=timeout
```

### 3. Metrics & Analytics
```bash
# Overall system metrics
GET /api/v1/metrics/overview

# Service-specific metrics
GET /api/v1/metrics/service/payment-service

# System-wide metrics
GET /api/v1/metrics/system

# Time series data
GET /api/v1/metrics/timeseries?interval=5m
```

**Metrics Include:**
- Total log count
- Logs per minute/second
- Error rates by service
- Log distribution by level
- Top error messages
- Service health indicators

### 4. Smart Alerting

Create rules that trigger notifications:
```json
{
  "name": "High Error Rate",
  "condition": "greater_than",
  "threshold": 100,
  "levels": ["ERROR"],
  "notification_channel": "webhook"
}
```

**Alert Features:**
- Multiple conditions (>, <, =, >=, <=)
- Filter by service and log level
- Multiple notification channels
- Alert history and tracking
- Acknowledge/resolve workflow

### 5. Authentication & Authorization

- **JWT Tokens**: Secure token-based auth
- **Role-Based Access**: User and admin roles
- **Protected Endpoints**: Secure write operations
- **Password Hashing**: Bcrypt encryption

### 6. Redis Caching

- **Automatic**: Transparent caching on metrics endpoints
- **Fast**: 10-100x performance improvement
- **Configurable TTL**: Different cache times per endpoint
- **Graceful Degradation**: Works without Redis

### 7. Rate Limiting

- **Per-User**: Track authenticated users by ID
- **Per-IP**: Track anonymous users by IP
- **Configurable**: Different limits per endpoint
- **Fair Usage**: Prevents API abuse

---

## üì° API Endpoints

### Authentication

| Method | Endpoint | Rate Limit | Description |
|--------|----------|------------|-------------|
| POST | `/api/v1/auth/register` | 10/hour | Register new user |
| POST | `/api/v1/auth/login` | 20/hour | Login and get JWT token |
| GET | `/api/v1/auth/me` | 100/min | Get current user |
| PUT | `/api/v1/auth/me` | 30/hour | Update profile |
| GET | `/api/v1/auth/users` | 50/min | List all users (admin) |

### Logs

| Method | Endpoint | Rate Limit | Description |
|--------|----------|------------|-------------|
| GET | `/api/v1/logs/search` | 50/min | Search logs with filters |
| GET | `/api/v1/logs/recent` | 100/min | Get recent logs |
| GET | `/api/v1/logs/{id}` | 200/min | Get specific log by ID |

### Metrics

| Method | Endpoint | Cache TTL | Description |
|--------|----------|-----------|-------------|
| GET | `/api/v1/metrics/overview` | 30s | Overall metrics |
| GET | `/api/v1/metrics/service/{name}` | 60s | Service metrics |
| GET | `/api/v1/metrics/system` | 45s | System metrics |
| GET | `/api/v1/metrics/timeseries` | 120s | Time series data |

### Alerts

| Method | Endpoint | Auth Required | Description |
|--------|----------|---------------|-------------|
| POST | `/api/v1/alerts/rules` | ‚úÖ | Create alert rule |
| GET | `/api/v1/alerts/rules` | ‚ùå | List alert rules |
| GET | `/api/v1/alerts/rules/{id}` | ‚ùå | Get specific rule |
| PUT | `/api/v1/alerts/rules/{id}` | ‚úÖ Owner/Admin | Update rule |
| DELETE | `/api/v1/alerts/rules/{id}` | ‚úÖ Owner/Admin | Delete rule |
| GET | `/api/v1/alerts/triggered` | ‚ùå | List triggered alerts |
| POST | `/api/v1/alerts/triggered/{id}/acknowledge` | ‚úÖ | Acknowledge alert |
| POST | `/api/v1/alerts/triggered/{id}/resolve` | ‚úÖ | Resolve alert |

### Cache Management

| Method | Endpoint | Auth Required | Description |
|--------|----------|---------------|-------------|
| GET | `/api/v1/cache/stats` | ‚ùå | Cache statistics |
| DELETE | `/api/v1/cache/clear` | ‚úÖ Admin | Clear cache |

### Rate Limit Management

| Method | Endpoint | Auth Required | Description |
|--------|----------|---------------|-------------|
| GET | `/api/v1/rate-limits/status` | ‚úÖ Admin | View rate limit status |
| DELETE | `/api/v1/rate-limits/reset/{id}` | ‚úÖ Admin | Reset specific limit |
| DELETE | `/api/v1/rate-limits/reset-all` | ‚úÖ Admin | Reset all limits |

---

## üîê Authentication & Security

### Register & Login
```bash
# 1. Register new user
curl -X POST "http://localhost:8000/api/v1/auth/register" \
  -H "Content-Type: application/json" \
  -d '{
    "username": "alice",
    "email": "alice@example.com",
    "password": "secure123",
    "full_name": "Alice Smith"
  }'

# 2. Login and get token
TOKEN=$(curl -s -X POST "http://localhost:8000/api/v1/auth/login" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "username=alice&password=secure123" \
  | python3 -c "import sys, json; print(json.load(sys.stdin)['access_token'])")

# 3. Use token for authenticated requests
curl -X POST "http://localhost:8000/api/v1/alerts/rules" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{...}'
```

### Security Features

- ‚úÖ **Bcrypt Password Hashing**: Industry-standard encryption
- ‚úÖ **JWT Tokens**: Stateless authentication
- ‚úÖ **Token Expiration**: 30-minute default (configurable)
- ‚úÖ **Role-Based Access**: User and admin permissions
- ‚úÖ **Rate Limiting**: Prevent brute force attacks
- ‚úÖ **CORS Protection**: Configurable origins

---

## üö® Alerting System

### Create Alert Rule
```bash
POST /api/v1/alerts/rules
Authorization: Bearer <token>

{
  "name": "Critical Error Spike",
  "description": "Alert when errors exceed threshold",
  "condition": "greater_than",
  "threshold": 50,
  "time_window": 5,
  "levels": ["ERROR"],
  "services": ["payment-service"],
  "notification_channel": "webhook",
  "notification_config": {
    "url": "https://hooks.slack.com/..."
  },
  "is_active": true
}
```

### Alert Conditions

- `greater_than`: Value > threshold
- `less_than`: Value < threshold
- `equals`: Value == threshold
- `greater_than_or_equal`: Value >= threshold
- `less_than_or_equal`: Value <= threshold

### Notification Channels

- **Console**: Print to logs (development)
- **Webhook**: POST to HTTP endpoint
- **Email**: Send email (extensible)
- **Slack**: Slack webhooks (extensible)

### Alert Workflow

1. Engine checks rules every 60 seconds
2. Evaluates conditions against Elasticsearch
3. If triggered, creates `triggered_alert` record
4. Sends notification via configured channel
5. Updates rule statistics

---

## ‚ö° Caching

### Performance Impact

| Metric | Without Cache | With Cache | Improvement |
|--------|---------------|------------|-------------|
| Overview | 250ms | 8ms | **31x faster** |
| Service Metrics | 180ms | 12ms | **15x faster** |
| System Metrics | 400ms | 15ms | **27x faster** |

### Cache Configuration
```bash
# View cache stats
GET /api/v1/cache/stats

# Clear cache (admin only)
DELETE /api/v1/cache/clear?pattern=metrics:*
```

### Cache TTLs

- Overview: 30 seconds
- Service metrics: 60 seconds
- System metrics: 45 seconds
- Time series: 120 seconds

---

## üö¶ Rate Limiting

### Rate Limits by Endpoint

| Endpoint Type | Limit | Window |
|---------------|-------|--------|
| Default | 100 | per minute |
| Search | 50 | per minute |
| Recent Logs | 100 | per minute |
| Direct Lookup | 200 | per minute |
| Auth Login | 20 | per hour |
| Auth Register | 10 | per hour |
| Metrics | 100 | per minute |

### Testing Rate Limits
```bash
# Automated test
./scripts/test-rate-limit.sh

# Manual test
for i in {1..70}; do
  curl -s -o /dev/null -w "%{http_code} " http://localhost:8000/
done
```

### Rate Limit Response
```http
HTTP/1.1 429 Too Many Requests
Content-Type: application/json

{
  "error": "Rate limit exceeded: 60 per 1 minute"
}
```

---

## üóÑÔ∏è Database Management

### Quick Commands
```bash
# Initialize database
./scripts/manage-db.sh init

# Open PostgreSQL shell
./scripts/manage-db.sh shell

# Create backup
./scripts/manage-db.sh backup

# Reset database (‚ö†Ô∏è deletes all data!)
./scripts/manage-db.sh reset
```

### Database Schema

**Tables:**
- `users` - User accounts and authentication
- `alert_rules` - Alert configurations
- `triggered_alerts` - Alert history
- `system_config` - System settings

### Common Queries
```sql
-- List all users
SELECT username, email, is_admin FROM users;

-- List active alert rules
SELECT name, condition, threshold, is_active 
FROM alert_rules 
WHERE is_active = true;

-- Recent triggered alerts
SELECT r.name, a.triggered_at, a.status 
FROM triggered_alerts a 
JOIN alert_rules r ON a.rule_id = r.id 
ORDER BY a.triggered_at DESC 
LIMIT 10;
```

---

## üìä Monitoring

### Service Health
```bash
# Check all services
curl http://localhost:8000/health | python3 -m json.tool

# View cache statistics
curl http://localhost:8000/api/v1/cache/stats | python3 -m json.tool

# Check rate limit status (admin only)
curl -H "Authorization: Bearer $TOKEN" \
  http://localhost:8000/api/v1/rate-limits/status
```

### Docker Services
```bash
# View all services
docker compose ps

# Check service logs
docker compose logs api
docker compose logs alerting
docker compose logs elasticsearch

# Resource usage
docker stats
```

### Performance Metrics
```bash
# Elasticsearch cluster health
curl http://localhost:9200/_cluster/health?pretty

# Redis info
docker compose exec redis redis-cli INFO

# PostgreSQL connections
docker compose exec postgres psql -U logflow -d logflow \
  -c "SELECT count(*) FROM pg_stat_activity;"
```

---

## üõ†Ô∏è Development

### Project Structure
```
logflow/
‚îú‚îÄ‚îÄ api/                      # REST API
‚îÇ   ‚îú‚îÄ‚îÄ routes/              # API endpoints
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Pydantic & SQLAlchemy models
‚îÇ   ‚îú‚îÄ‚îÄ utils/               # Utilities (DB, ES, Redis, Auth)
‚îÇ   ‚îî‚îÄ‚îÄ config.py            # Configuration
‚îú‚îÄ‚îÄ alerting/                # Alert engine
‚îÇ   ‚îú‚îÄ‚îÄ engine.py            # Main alert loop
‚îÇ   ‚îú‚îÄ‚îÄ alert_evaluator.py  # Rule evaluation
‚îÇ   ‚îî‚îÄ‚îÄ notifier.py          # Notifications
‚îú‚îÄ‚îÄ producer/                # Log shipper (Kafka producer)
‚îú‚îÄ‚îÄ consumer/                # Log processor (Kafka consumer)
‚îú‚îÄ‚îÄ log_generator/           # Synthetic log generation
‚îú‚îÄ‚îÄ scripts/                 # Utility scripts
‚îú‚îÄ‚îÄ docker-compose.yml       # Service orchestration
‚îú‚îÄ‚îÄ Dockerfile.*             # Container definitions
‚îî‚îÄ‚îÄ requirements.txt         # Python dependencies
```

### Local Development
```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Run tests
pytest

# Run specific service locally
python -m api.main
python -m alerting.engine
```

### Environment Variables

Create `.env` for local development:
```bash
# Copy Docker environment
cp .env.docker .env

# Update for local use
POSTGRES_HOST=127.0.0.1
KAFKA_BOOTSTRAP_SERVERS=localhost:9092
ELASTICSEARCH_HOSTS=http://localhost:9200
REDIS_HOST=localhost
```

---

## üß™ Testing

### Automated Tests
```bash
# Test authentication
./scripts/test-auth.sh

# Test caching
./scripts/test-cache.sh

# Test rate limiting
./scripts/test-rate-limit.sh

# Test log pipeline
./scripts/pipeline-status.sh
```

### Manual Testing
```bash
# Generate test logs
python log_generator/generator.py --duration 60

# Test search
curl "http://localhost:8000/api/v1/logs/search?level=ERROR&limit=10"

# Test metrics
curl http://localhost:8000/api/v1/metrics/overview

# Test alerts
curl -X POST http://localhost:8000/api/v1/alerts/rules \
  -H "Authorization: Bearer $TOKEN" \
  -d '{...}'
```

---

## üöÄ Deployment

### Docker Compose (Current)
```bash
# Start everything
./scripts/start-all-docker.sh

# Stop everything
docker compose down

# Stop and remove volumes
docker compose down -v
```

### Production Checklist

- [ ] Change default admin password
- [ ] Set strong SECRET_KEY in environment
- [ ] Configure proper CORS origins
- [ ] Enable HTTPS/TLS
- [ ] Set up log rotation
- [ ] Configure backups
- [ ] Set up monitoring alerts
- [ ] Review rate limits
- [ ] Enable Elasticsearch security
- [ ] Set up firewall rules

### Environment Configuration

**Production `.env.docker`:**
```bash
SECRET_KEY=<generate-with-openssl-rand-hex-32>
ENVIRONMENT=production
POSTGRES_PASSWORD=<strong-password>
ACCESS_TOKEN_EXPIRE_MINUTES=15
```

---

## üîß Troubleshooting

### Common Issues

**1. Services not starting**
```bash
# Check Docker resources
docker system df

# Clean up
docker system prune -a --volumes

# Restart services
docker compose up -d
```

**2. Elasticsearch yellow health**
```bash
# Normal for single-node development
# Check status
curl http://localhost:9200/_cluster/health?pretty
```

**3. API connection errors**
```bash
# Check API logs
docker compose logs api --tail 50

# Rebuild API
docker compose build --no-cache api
docker compose up -d api
```

**4. Database connection issues**
```bash
# Check PostgreSQL
docker compose exec postgres psql -U logflow -d logflow -c "SELECT 1;"

# Reinitialize
./scripts/manage-db.sh reset
```

**5. Redis not connected**
```bash
# Check Redis
docker compose exec redis redis-cli PING

# Restart Redis
docker compose restart redis
```

### Logs & Debugging
```bash
# View all logs
docker compose logs -f

# View specific service
docker compose logs -f api

# Check service status
docker compose ps

# Inspect container
docker inspect logflow-api
```

---

## üìö Additional Resources

- **API Documentation**: http://localhost:8000/docs
- **Elasticsearch**: http://localhost:9200
- **Kibana**: http://localhost:5601
- **Database Guide**: `docs/DATABASE.md`

---

## üéì Technology Stack

| Component | Technology | Version |
|-----------|-----------|---------|
| Language | Python | 3.11+ |
| API Framework | FastAPI | 0.104+ |
| Message Queue | Apache Kafka | 7.5 |
| Search Engine | Elasticsearch | 8.11 |
| Database | PostgreSQL | 16 |
| Cache | Redis | 7 |
| ORM | SQLAlchemy | 2.0 |
| Auth | JWT (python-jose) | 3.3 |
| Password Hash | bcrypt | 4.0 |
| Rate Limiting | slowapi | 0.1.9 |
| Containerization | Docker Compose | - |

---

## üìä Project Statistics

- **Lines of Code**: ~5,000+
- **API Endpoints**: 30+
- **Docker Services**: 7
- **Database Tables**: 4
- **Test Scripts**: 5
- **Documentation**: Comprehensive

---

## ü§ù Contributing

This is a portfolio/learning project. Feel free to fork and extend!

---

## üìÑ License

MIT License - See LICENSE file for details

---

## üë®‚Äçüíª Author

Built by [Your Name]

- GitHub: [@yourusername]
- Email: your.email@example.com
- LinkedIn: [Your Profile]

---

## ‚≠ê Acknowledgments

Built as a demonstration of:
- Distributed systems architecture
- Real-time data processing
- RESTful API design
- Authentication & security
- Caching strategies
- Rate limiting
- Docker containerization
- Production-ready practices

---

**üéâ LogFlow - Production-Ready Log Aggregation**

If you found this project helpful, please star the repository!

## üìä Prometheus Metrics

LogFlow exports metrics in Prometheus format for monitoring and observability.

### Metrics Endpoints

- **Application Metrics**: `http://localhost:8000/metrics`
- **FastAPI Metrics**: `http://localhost:8000/metrics/fastapi`

### Available Metrics

#### HTTP Metrics
```
http_requests_total{method, endpoint, status}          # Total requests
http_request_duration_seconds{method, endpoint}        # Request latency
```

#### Business Metrics
```
logs_processed_total                                   # Logs processed
logs_indexed_total{service, level}                     # Logs indexed
elasticsearch_queries_total{operation}                 # ES queries
elasticsearch_query_duration_seconds{operation}        # ES query time
```

#### Alert Metrics
```
alerts_triggered_total{rule_name}                      # Alerts triggered
alerts_checked_total                                   # Alert checks
```

#### Cache Metrics
```
cache_hits_total{cache_key_prefix}                     # Cache hits
cache_misses_total{cache_key_prefix}                   # Cache misses
```

#### Database Metrics
```
database_queries_total{operation}                      # DB queries
database_connections                                   # Active connections
```

#### Authentication Metrics
```
auth_attempts_total{result}                            # Auth attempts
users_registered_total                                 # Users registered
```

#### Rate Limit Metrics
```
rate_limit_exceeded_total{endpoint}                    # Rate limits hit
```

### Scraping with Prometheus

Add to your `prometheus.yml`:
```yaml
scrape_configs:
  - job_name: 'logflow'
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/metrics'
    scrape_interval: 15s
```

### Example Queries
```promql
# Request rate per endpoint
rate(http_requests_total[5m])

# 95th percentile response time
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Error rate
rate(http_requests_total{status=~"5.."}[5m])

# Cache hit rate
rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m]))

# Alert trigger rate
rate(alerts_triggered_total[1h])
```

### Grafana Dashboard

Import the included Grafana dashboard for visualization:

1. Open Grafana
2. Import dashboard
3. Use `grafana-dashboard.json` from the project
4. Select Prometheus data source

### Testing Metrics
```bash
# Run metrics test
./scripts/test-metrics.sh

# View metrics in browser
open http://localhost:8000/metrics

# Generate traffic for testing
for i in {1..100}; do
  curl -s http://localhost:8000/ > /dev/null
  curl -s http://localhost:8000/api/v1/metrics/overview > /dev/null
done

# View updated metrics
curl http://localhost:8000/metrics | grep http_requests_total
```


## üîç Request Tracing & Correlation

LogFlow implements distributed request tracing to track requests across the system.

### Request IDs

Every request gets a unique ID automatically:
```bash
curl -i http://localhost:8000/

# Response headers include:
X-Request-ID: 550e8400-e29b-41d4-a716-446655440000
X-Correlation-ID: 550e8400-e29b-41d4-a716-446655440000
```

### Correlation IDs

Track related requests across services:
```bash
# Send custom correlation ID
curl -H "X-Correlation-ID: my-transaction-123" \
  http://localhost:8000/api/v1/logs/search

# The correlation ID is preserved in the response
X-Correlation-ID: my-transaction-123
```

### Trace Context API

**Get current trace context:**
```bash
GET /api/v1/trace/context

Response:
{
  "request_id": "550e8400-e29b-41d4-a716-446655440000",
  "correlation_id": "550e8400-e29b-41d4-a716-446655440000",
  "timestamp": "2025-10-24T08:50:00.000000"
}
```

**Get debug trace (admin only):**
```bash
GET /api/v1/trace/debug
Authorization: Bearer <token>

Response:
{
  "request_id": "550e8400-e29b-41d4-a716-446655440000",
  "correlation_id": "my-transaction-123",
  "method": "GET",
  "path": "/api/v1/trace/debug",
  "headers": {...},
  "query_params": {...},
  "timestamp": "2025-10-24T08:50:00.000000",
  "user": "admin"
}
```

**Log custom trace event:**
```bash
POST /api/v1/trace/log
{
  "event": "payment_processed",
  "details": {
    "amount": 99.99,
    "status": "success"
  }
}
```

### Using Trace Context in Code
```python
from api.middleware.request_id import get_request_id, get_correlation_id

# Get current request ID
request_id = get_request_id()

# Get correlation ID
correlation_id = get_correlation_id()

# Log with context
logger.bind(
    request_id=request_id,
    correlation_id=correlation_id
).info("Processing payment")
```

### Trace Headers Flow
```
Client Request
  ‚Üì (X-Correlation-ID: abc-123)
API Gateway
  ‚Üì (X-Request-ID: req-456)
  ‚Üì (X-Correlation-ID: abc-123)
Service A
  ‚Üì (X-Request-ID: req-789)
  ‚Üì (X-Correlation-ID: abc-123)
Service B
  ‚Üì Response includes both IDs
Client
```

### Log Correlation Example
```bash
# Start a transaction
CORRELATION_ID="txn-$(date +%s)"

# Make multiple related requests
curl -H "X-Correlation-ID: $CORRELATION_ID" \
  http://localhost:8000/api/v1/auth/login

curl -H "X-Correlation-ID: $CORRELATION_ID" \
  http://localhost:8000/api/v1/logs/search

curl -H "X-Correlation-ID: $CORRELATION_ID" \
  http://localhost:8000/api/v1/alerts/rules

# All logs will have the same correlation_id
# Search logs: grep "$CORRELATION_ID" logs/*.log
```

### Testing Tracing
```bash
# Run tracing test
./scripts/test-tracing.sh

# Test with custom correlation ID
CORR_ID="my-test-$(date +%s)"
curl -H "X-Correlation-ID: $CORR_ID" \
  http://localhost:8000/ | python3 -m json.tool

# Verify correlation ID in response headers
curl -i -H "X-Correlation-ID: $CORR_ID" \
  http://localhost:8000/ | grep -i correlation
```

### Benefits

- **Debugging**: Track requests across services
- **Performance**: Identify slow request paths
- **Monitoring**: Correlate logs and metrics
- **Troubleshooting**: Follow user transactions
- **Analytics**: Analyze user journeys

### Best Practices

1. **Always preserve correlation IDs** across service boundaries
2. **Include IDs in all logs** for easy searching
3. **Use correlation IDs for transactions** spanning multiple requests
4. **Add trace context to error reports** for debugging
5. **Monitor request chains** to identify bottlenecks


---

## ü§ñ ML-Based Anomaly Detection

LogFlow uses machine learning to automatically detect unusual patterns in your logs, helping you catch issues before they become critical.

### Detection Algorithms

LogFlow employs **three complementary methods** for robust anomaly detection:

1. **Z-Score (Statistical)**
   - Detects deviations from historical mean
   - Threshold: 1.0 standard deviations
   - Best for: Sudden spikes or drops

2. **Moving Average (Trend-based)**
   - Compares current values to rolling average
   - Adaptive window size
   - Best for: Gradual trend changes

3. **Isolation Forest (Machine Learning)**
   - scikit-learn ensemble algorithm
   - Detects complex patterns
   - Best for: Multi-dimensional anomalies

### API Endpoints

**Detect log volume anomalies:**
```bash
GET /api/v1/anomaly/detect/log-volume?window_minutes=60

Response:
[
  {
    "detected_at": "2025-10-26T16:22:00.000Z",
    "metric_name": "log_volume",
    "service": "payment-service",
    "anomaly_type": "spike",
    "description": "log_volume spike: 3000.00 (expected ~225.00)",
    "score": 2.25,
    "severity": "high",
    "actual_value": 3000.0,
    "expected_value": 225.0,
    "deviation_percent": 1233.3
  }
]
```

**Detect error rate anomalies:**
```bash
GET /api/v1/anomaly/detect/error-rate?window_minutes=60&service=auth-service
```

**Get comprehensive anomaly report:**
```bash
GET /api/v1/anomaly/report?window_minutes=60

Response:
{
  "period_start": "2025-10-26T16:00:00",
  "period_end": "2025-10-26T17:00:00",
  "total_anomalies": 5,
  "anomalies": [...],
  "anomalies_by_severity": {
    "critical": 2,
    "high": 1,
    "medium": 2
  },
  "anomalies_by_service": {
    "payment-service": 3,
    "auth-service": 2
  }
}
```

### Anomaly Types

| Type | Description | Example |
|------|-------------|---------|
| **spike** | Sudden increase (>100%) | DDoS attack, traffic surge |
| **drop** | Sudden decrease | Service outage, network issue |
| **pattern_change** | Unusual pattern detected by ML | Degraded performance, subtle bugs |

### Severity Levels

| Severity | Z-Score | Deviation | Action |
|----------|---------|-----------|--------|
| **Critical** | > 4.0 | > 400% | Immediate action required |
| **High** | 3.0-4.0 | 300-400% | Urgent investigation |
| **Medium** | 2.5-3.0 | 250-300% | Review soon |
| **Low** | 1.0-2.5 | 100-250% | Monitor |

### Configuration

Anomaly detection can be configured via query parameters:
```bash
# Time window (10-1440 minutes)
?window_minutes=60

# Service-specific detection
?service=payment-service

# Combined
?window_minutes=120&service=auth-service
```

### Use Cases

**1. DDoS Attack Detection**
```bash
# Detect sudden log volume spikes
curl "http://localhost:8000/api/v1/anomaly/detect/log-volume?window_minutes=10"

# If spike > 500%, likely DDoS
```

**2. Service Degradation**
```bash
# Monitor error rate changes
curl "http://localhost:8000/api/v1/anomaly/detect/error-rate?window_minutes=30"

# Rising error rates indicate problems
```

**3. Capacity Planning**
```bash
# Analyze trends over 24 hours
curl "http://localhost:8000/api/v1/anomaly/report?window_minutes=1440"

# Identify peak usage patterns
```

**4. Security Incidents**
```bash
# Detect unusual authentication patterns
curl "http://localhost:8000/api/v1/anomaly/detect/log-volume?service=auth-service&window_minutes=60"

# Spikes may indicate brute force attacks
```

**5. Performance Monitoring**
```bash
# Track all services
curl "http://localhost:8000/api/v1/anomaly/report?window_minutes=60"

# Group by service to find bottlenecks
```

### Testing Anomaly Detection
```bash
# Run comprehensive test
./scripts/test-anomaly-complete.sh

# Output:
# ‚úÖ Detected 3 log volume anomalies
#    - HIGH: 3000 logs (expected 225, 1233.3% deviation)
```

### How It Works

1. **Data Collection**: Aggregates logs into 1-minute buckets
2. **Statistical Analysis**: Calculates mean, std dev, moving averages
3. **ML Processing**: Isolation Forest identifies outliers
4. **Scoring**: Assigns severity based on deviation magnitude
5. **Deduplication**: Removes overlapping detections, keeps highest severity
6. **Reporting**: Returns ranked anomalies with metadata

### Algorithm Details

**Z-Score Method:**
```python
z_score = (actual_value - mean) / std_deviation

if z_score > 1.0:
    # Anomaly detected
    severity = calculate_severity(z_score)
```

**Moving Average:**
```python
moving_avg = average(last_5_values)
deviation = |current_value - moving_avg|

if deviation > threshold * moving_std:
    # Anomaly detected
```

**Isolation Forest:**
```python
clf = IsolationForest(contamination=0.1)
predictions = clf.fit_predict(values)

# -1 indicates anomaly
anomalies = values[predictions == -1]
```

### Best Practices

1. **Choose appropriate time windows**
   - Short windows (10-30 min): Real-time detection
   - Medium windows (60-120 min): Pattern analysis
   - Long windows (12-24 hours): Trend analysis

2. **Set up alerts for critical anomalies**
```bash
   # Monitor and alert on critical anomalies
   while true; do
     CRITICAL=$(curl -s "localhost:8000/api/v1/anomaly/detect/log-volume?window_minutes=10" | \
       grep '"severity": "critical"' | wc -l)
     if [ $CRITICAL -gt 0 ]; then
       echo "ALERT: $CRITICAL critical anomalies detected!"
       # Send notification
     fi
     sleep 60
   done
```

3. **Review false positives**
   - Legitimate traffic spikes (marketing campaigns, sales)
   - Scheduled batch jobs
   - Deploy new features gradually to establish new baselines

4. **Combine with correlation tracking**
```bash
   # Find anomaly then trace the flow
   ANOMALY_TIME="2025-10-26T16:22:00.000Z"
   
   # Find correlation IDs around that time
   curl "localhost:8000/api/v1/logs/search?timestamp=$ANOMALY_TIME&limit=10"
   
   # Trace the full request flow
   curl "localhost:8000/api/v1/correlation/trace/{correlation_id}"
```

5. **Service-specific baselines**
   - Different services have different normal patterns
   - Use service parameter for accurate detection
   - Auth services: Low volume, very sensitive
   - API gateways: High volume, less sensitive

### Performance

- **Detection speed**: < 500ms for 60-minute window
- **Data points**: Handles 100+ time buckets efficiently
- **Memory usage**: < 100MB for large datasets
- **ML model**: Pre-trained Isolation Forest (no training delay)

### Limitations

- **Minimum samples**: Requires 5+ data points
- **Cold start**: Needs historical data for baselines
- **Seasonal patterns**: May flag legitimate weekly/daily patterns
- **Threshold tuning**: Default sensitivity (1.0) may need adjustment

### Future Enhancements

- [ ] Automatic baseline learning
- [ ] Seasonal pattern detection
- [ ] Anomaly prediction (forecasting)
- [ ] Custom thresholds per service
- [ ] Anomaly correlation across services
- [ ] Integration with alerting system

### Machine Learning Stack

- **scikit-learn**: Isolation Forest algorithm
- **numpy**: Statistical operations (mean, std, z-score)
- **scipy**: Statistical tests and distributions
- **pandas**: Time series analysis (future)

### Troubleshooting

**No anomalies detected:**
```bash
# Check if you have recent data
curl "http://localhost:9200/logs/_count?q=timestamp:[now-1h TO now]"

# Try wider window
curl "localhost:8000/api/v1/anomaly/detect/log-volume?window_minutes=120"

# Check sensitivity (lower = more sensitive)
# Edit api/services/anomaly_detector.py: sensitivity = 0.5
```

**Too many false positives:**
```bash
# Increase sensitivity threshold
# Edit api/services/anomaly_detector.py: sensitivity = 1.5

# Or filter by severity
curl "localhost:8000/api/v1/anomaly/report" | jq '.anomalies[] | select(.severity == "critical")'
```

**Slow detection:**
```bash
# Reduce time window
?window_minutes=30

# Or reduce aggregation resolution (1m -> 5m)
# Edit _get_log_volume_timeseries: fixed_interval: "5m"
```

### Example Scenarios

**Scenario 1: Traffic Spike Detection**
```bash
# Normal: 100 logs/min
# Spike: 2000 logs/min
# Detection: 
{
  "severity": "critical",
  "deviation_percent": 1900,
  "actual_value": 2000,
  "expected_value": 100
}
# Action: Scale infrastructure, check for DDoS
```

**Scenario 2: Service Degradation**
```bash
# Normal error rate: 0.5%
# Degraded: 15%
# Detection:
{
  "metric_name": "error_rate",
  "severity": "high",
  "deviation_percent": 2900
}
# Action: Check recent deploys, investigate errors
```

**Scenario 3: Overnight Batch Job**
```bash
# Daily 2am spike (legitimate)
# Solution: Exclude time ranges or use longer baseline window
?window_minutes=1440  # 24 hours includes pattern
```


---

## üíª React Dashboard

LogFlow includes a modern, responsive web dashboard built with **Next.js 14, React, and Tailwind CSS** for real-time log monitoring and analytics.

### Features

- **üìä Real-time Metrics**: Live overview of log volume, error rates, and service health
- **üîç Advanced Search**: Full-text search with filters (service, level, time range)
- **üìã Log Table**: Sortable, filterable table with log details
- **üé® Modern UI**: Clean, responsive design with Tailwind CSS
- **‚ö° Fast Performance**: Next.js with Turbopack for instant page loads

### Getting Started
```bash
# Navigate to dashboard
cd dashboard

# Install dependencies
npm install

# Set API URL (optional - defaults to localhost:8000)
echo "NEXT_PUBLIC_API_URL=http://localhost:8000" > .env.local

# Start development server
npm run dev

# Open browser at http://localhost:3000
```

### Dashboard Pages

#### **Main Dashboard** (`/`)

The main dashboard provides an at-a-glance view of your log system:

**Metrics Cards:**
- **Total Logs**: All-time log count
- **Logs/Minute**: Current ingestion rate with trend indicator
- **Active Services**: Number of services sending logs
- **Error Rate**: Percentage of ERROR/CRITICAL logs with trend

**Search Interface:**
- Full-text search across all log fields
- Filter by service (auth, payment, order, notification, etc.)
- Filter by log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
- Time range selector (15m, 1h, 6h, 24h, 7d)

**Recent Logs Table:**
- Timestamp with human-readable formatting
- Color-coded log levels (blue=INFO, yellow=WARNING, red=ERROR, purple=CRITICAL)
- Service name
- Log message (truncated for readability)
- Correlation ID (for distributed tracing)
- Hover effect for better UX

### Technology Stack

| Technology | Purpose |
|------------|---------|
| **Next.js 14** | React framework with App Router |
| **TypeScript** | Type safety and better DX |
| **Tailwind CSS** | Utility-first styling |
| **Axios** | HTTP client for API calls |
| **Recharts** | Charts and data visualization (ready) |
| **Lucide React** | Modern icon library |
| **date-fns** | Date formatting and manipulation |

### Project Structure
```
dashboard/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx          # Root layout with metadata
‚îÇ   ‚îú‚îÄ‚îÄ page.tsx            # Main dashboard page
‚îÇ   ‚îî‚îÄ‚îÄ globals.css         # Global styles with Tailwind
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ MetricsCard.tsx     # Metric display card
‚îÇ   ‚îú‚îÄ‚îÄ LogSearch.tsx       # Search and filter interface
‚îÇ   ‚îî‚îÄ‚îÄ LogTable.tsx        # Log table with styling
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îî‚îÄ‚îÄ api.ts              # API client and endpoints
‚îú‚îÄ‚îÄ .env.local              # Environment variables
‚îú‚îÄ‚îÄ next.config.js          # Next.js configuration
‚îú‚îÄ‚îÄ tailwind.config.js      # Tailwind CSS configuration
‚îî‚îÄ‚îÄ package.json            # Dependencies
```

### API Integration

The dashboard connects to LogFlow API endpoints:
```typescript
// lib/api.ts

// Search logs
const logs = await searchLogs({
  query: 'error payment',
  service: 'payment-service',
  level: 'ERROR',
  limit: 100
});

// Get metrics overview
const metrics = await getMetricsOverview();

// Get service-specific metrics
const serviceMetrics = await getServiceMetrics(60); // last 60 minutes

// Get log volume over time
const volume = await getLogVolume(60);
```

### Components

#### **MetricsCard**

Reusable metric display component with trend indicators:
```tsx
<MetricsCard
  title="Total Logs"
  value="1,234,567"
  icon={<Database />}
  trend={{ value: 12.5, isPositive: true }}
  subtitle="All time"
/>
```

#### **LogSearch**

Advanced search interface with filters:
```tsx
<LogSearch onSearch={(params) => handleSearch(params)} />
```

Features:
- Real-time search
- Service dropdown
- Log level filter
- Time range selector
- Export functionality (ready)

#### **LogTable**

Responsive log table with:
- Color-coded log levels
- Sortable columns
- Hover effects
- Correlation ID display
- Timestamp formatting
- Loading and empty states

### Styling

The dashboard uses Tailwind CSS with custom configuration:
```javascript
// tailwind.config.js
module.exports = {
  content: [
    "./app/**/*.{js,ts,jsx,tsx,mdx}",
    "./components/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      // Custom colors, fonts, etc.
    },
  },
}
```

**Color Scheme:**
- Primary: Blue (`blue-600`)
- Success: Green (`green-600`)
- Warning: Yellow (`yellow-600`)
- Error: Red (`red-600`)
- Critical: Purple (`purple-600`)

### Configuration

**Environment Variables:**
```bash
# .env.local
NEXT_PUBLIC_API_URL=http://localhost:8000  # API endpoint
```

**Next.js Config:**
```javascript
// next.config.js
const nextConfig = {
  reactStrictMode: true,
};
```

### Development
```bash
# Start development server with hot reload
npm run dev

# Build for production
npm run build

# Start production server
npm run start

# Lint code
npm run lint
```

### Browser Support

- Chrome/Edge 90+
- Firefox 88+
- Safari 14+
- Mobile browsers (iOS Safari, Chrome Android)

### Performance

- **First Load**: ~1.5s
- **Page Transitions**: Instant (client-side)
- **API Response Time**: <100ms (local)
- **Bundle Size**: ~200KB (gzipped)

### Future Enhancements

Planned features for the dashboard:

- [ ] **Real-time Charts**: Log volume over time with Recharts
- [ ] **Anomaly Visualization**: Visual anomaly detection panel
- [ ] **Live Updates**: WebSocket integration for streaming logs
- [ ] **Service Dependency Graph**: Interactive network visualization
- [ ] **Dark Mode**: Theme toggle
- [ ] **Alert Management**: Create/edit/delete alert rules via UI
- [ ] **User Authentication**: Login/logout with JWT
- [ ] **Export Functionality**: CSV/JSON export
- [ ] **Log Details Modal**: Click log for full details
- [ ] **Correlation Trace View**: Visual request flow diagram
- [ ] **Custom Dashboards**: User-configurable layouts
- [ ] **Saved Searches**: Bookmark common queries
- [ ] **Notifications**: Browser notifications for critical alerts

### Troubleshooting

**Dashboard won't start:**
```bash
# Clear cache and reinstall
rm -rf node_modules .next
npm install
npm run dev
```

**API connection errors:**
```bash
# Check API is running
curl http://localhost:8000/health

# Check environment variable
cat .env.local

# Test API endpoint
curl http://localhost:8000/api/v1/logs/recent?limit=5
```

**Styling not loading:**
```bash
# Rebuild Tailwind
npm run dev

# Hard refresh browser (Ctrl+Shift+R)
```

**TypeScript errors:**
```bash
# Check types
npx tsc --noEmit

# Update types
npm install --save-dev @types/node @types/react @types/react-dom
```

### Deployment

**Production Build:**
```bash
cd dashboard
npm run build
npm run start
```

**Docker Deployment:**
```dockerfile
# Dockerfile for dashboard (future)
FROM node:20-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --production
COPY . .
RUN npm run build
EXPOSE 3000
CMD ["npm", "start"]
```

**Environment Variables for Production:**
```bash
NEXT_PUBLIC_API_URL=https://api.logflow.io
NODE_ENV=production
```

### Security

- **XSS Protection**: React escapes content by default
- **CSRF**: Next.js built-in protection
- **API Security**: Uses same-origin policy
- **Environment Variables**: Never expose secrets in client-side code

### Accessibility

The dashboard follows WCAG 2.1 guidelines:
- ‚úÖ Semantic HTML
- ‚úÖ Keyboard navigation
- ‚úÖ ARIA labels
- ‚úÖ Color contrast ratios
- ‚úÖ Focus indicators
- ‚úÖ Screen reader support

### Screenshots

**Main Dashboard:**
- Clean, modern interface
- Metric cards with trends
- Search bar with filters
- Sortable log table
- Responsive design

**Search Interface:**
- Full-text search
- Multi-filter support
- Real-time results
- Export options

**Log Table:**
- Color-coded levels
- Hover effects
- Correlation tracking
- Pagination ready

---

## üöÄ Quick Start Guide

### Complete Setup (Backend + Frontend)
```bash
# 1. Start backend services
docker compose up -d

# 2. Start log generator (optional)
python3 log_generator/generator.py &

# 3. Start consumer (optional for fresh logs)
python3 consumer/consumer.py &

# 4. Start dashboard
cd dashboard
npm install
npm run dev

# 5. Open browser
# Backend API: http://localhost:8000
# Dashboard: http://localhost:3000
```

### Verify Everything Works
```bash
# Check API health
curl http://localhost:8000/health

# Check metrics
curl http://localhost:8000/api/v1/metrics/overview

# Check logs
curl http://localhost:8000/api/v1/logs/recent?limit=5
```

Then visit **http://localhost:3000** to see the dashboard! üéâ

